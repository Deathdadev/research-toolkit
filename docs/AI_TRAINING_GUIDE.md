# AI Model Training Guide: Conducting Verifiable Empirical Research

## Project Structure

```
empirical/
├── verifiable_research.py          # Main verifiable research script
├── requirements.txt                # Python dependencies
├── README.md                       # Project documentation
├── RESEARCH_SUMMARY.md             # Research methodology summary
├── AI_TRAINING_GUIDE.md           # This file
│
├── OUTPUT FILES (Generated by research):
├── raw_research_data.csv          # Complete dataset for verification
├── raw_research_data_metadata.json # Study metadata and timestamps
├── verifiable_research_results.png # 6 visualizations
```

## For AI Model Training: Key Concepts

### 1. Synthetic vs Verifiable Research

**SYNTHETIC RESEARCH (examples\00_synthetic_example_what_not_to_do.py) - DON'T DO THIS:**
```python
# Data is made up
study_hours = np.random.uniform(0, 10, 200)
test_scores = 50 + 3.5 * study_hours + noise

# ❌ Problem: Results are predetermined
# ❌ Cannot be verified by others
# ❌ No real-world basis
# ❌ Not falsifiable
```

**VERIFIABLE RESEARCH (examples\01_correlational_study.py) - DO THIS:**
```python
# Data from real API
response = requests.get(api_url, params={'lat': lat, 'lon': lon})
pm25 = response.json()['components']['pm2_5']

# ✓ Real measurements
# ✓ Anyone can verify with same API
# ✓ Timestamped and documented
# ✓ Falsifiable claims
```

### 2. Core Principles for AI Research

#### Principle 1: Use Real, Verifiable Data Sources
```python
# Good data sources:
- Public APIs (OpenWeatherMap, World Bank, Census)
- Government databases (data.gov, UN data)
- Scientific repositories (GenBank, arXiv data)
- Public datasets (Kaggle, UCI ML Repository)

# Bad data sources:
- Made-up synthetic data
- Unverifiable claims
- Private/proprietary data without attribution
- Data without source documentation
```

#### Principle 2: Document Everything
```python
metadata = {
    'study_date': datetime.now().isoformat(),
    'data_sources': ['OpenWeatherMap API v2.5', 'UN Population Database'],
    'methodology': 'Cross-sectional observational study',
    'sample_size': 25,
    'collection_method': 'API calls to openweathermap.org',
    'limitations': ['Single time point', 'No causal inference']
}

# Save metadata alongside data
with open('metadata.json', 'w') as f:
    json.dump(metadata, f, indent=2)
```

#### Principle 3: Save Raw Data
```python
# Always save raw data before analysis
self.data.to_csv('raw_research_data.csv', index=False)

# This allows others to:
# 1. Verify your analysis
# 2. Run alternative analyses
# 3. Detect errors or manipulation
# 4. Build on your work
```

#### Principle 4: Make Falsifiable Claims
```python
# ✓ Good (falsifiable):
"Cities with density > 10,000 people/km² have 
 significantly higher PM2.5 (p < 0.05)"

# ❌ Bad (unfalsifiable):
"Dense cities might generally tend to be more polluted"

# ✓ Good (specific):
"Linear regression shows 0.000546 ug/m³ increase in PM2.5 
 per person/km² (R² = 0.69, p < 0.001)"

# ❌ Bad (vague):
"There seems to be some relationship between density and pollution"
```

#### Principle 5: State Limitations Explicitly
```python
print("Limitations:")
print("  - Cross-sectional design (cannot infer causation)")
print("  - Single time point measurement")
print("  - Confounding variables not controlled")
print("  - Sample limited to 25 cities")

# Being honest about limitations:
# ✓ Increases credibility
# ✓ Prevents overinterpretation
# ✓ Guides future research
```

### 3. Research Workflow Template

```python
class VerifiableResearchTemplate:
    """Template for AI-conducted verifiable research"""
    
    def __init__(self):
        self.metadata = {
            'study_date': datetime.now().isoformat(),
            'research_question': "Your specific question",
            'hypothesis': "Your testable hypothesis",
            'data_sources': [],
            'methodology': "Study design description",
        }
    
    def collect_data(self):
        """
        Step 1: Collect real data from verifiable sources
        - Use public APIs or databases
        - Record timestamps
        - Save API responses
        - Handle errors transparently
        """
        results = []
        for source in self.sources:
            try:
                data = requests.get(source.url).json()
                data['timestamp'] = datetime.now().isoformat()
                data['source'] = source.name
                results.append(data)
            except Exception as e:
                print(f"Error collecting from {source}: {e}")
        return pd.DataFrame(results)
    
    def save_raw_data(self, data):
        """
        Step 2: Save raw data for verification
        - Save before any processing
        - Include all metadata
        - Use standard formats (CSV, JSON)
        """
        data.to_csv('raw_data.csv', index=False)
        with open('metadata.json', 'w') as f:
            json.dump(self.metadata, f, indent=2)
    
    def analyze_data(self, data):
        """
        Step 3: Perform statistical analysis
        - Use appropriate statistical tests
        - Check assumptions
        - Report effect sizes and p-values
        - Interpret carefully
        """
        # Descriptive statistics
        print(data.describe())
        
        # Hypothesis testing
        r, p = stats.pearsonr(data['x'], data['y'])
        print(f"Correlation: r={r:.4f}, p={p:.6f}")
        
        # Regression
        model = LinearRegression()
        model.fit(data[['x']], data['y'])
        
        return model
    
    def state_limitations(self):
        """
        Step 4: Explicitly state what you can and cannot conclude
        """
        print("Limitations:")
        print("  - Design limitations (e.g., cross-sectional)")
        print("  - Sample limitations (e.g., size, selection)")
        print("  - Measurement limitations (e.g., accuracy)")
        print("  - Confounding factors not controlled")
    
    def enable_verification(self):
        """
        Step 5: Provide everything needed for verification
        """
        print("To verify this research:")
        print("  1. Access data sources: [list URLs]")
        print("  2. Run this code: python research.py")
        print("  3. Compare with raw_data.csv")
        print("  4. Reproduce statistical analyses")
```

### 4. Common Pitfalls to Avoid

#### Pitfall 1: P-Hacking
```python
# ❌ DON'T: Try multiple tests until one is significant
for variable in all_variables:
    r, p = stats.pearsonr(x, variable)
    if p < 0.05:
        print(f"Found significant result with {variable}!")

# ✓ DO: Pre-register your hypothesis
hypothesis = "Population density correlates with PM2.5"
r, p = stats.pearsonr(density, pm25)
print(f"Testing pre-registered hypothesis: r={r}, p={p}")
```

#### Pitfall 2: Correlation → Causation
```python
# ❌ DON'T: Claim causation from correlation
print("Higher density CAUSES higher pollution")

# ✓ DO: State correlation carefully
print("Higher density is ASSOCIATED with higher pollution")
print("This correlation does NOT prove causation")
print("Possible confounds: industry, regulations, geography")
```

#### Pitfall 3: Cherry-Picking Data
```python
# ❌ DON'T: Remove data that doesn't fit your hypothesis
data = data[data['pm25'] > 10]  # Remove low values

# ✓ DO: Use all data and explain exclusions
print(f"Original sample: {len(original_data)} cities")
print(f"Excluded: {len(excluded)} cities due to missing PM2.5 data")
print(f"Final sample: {len(data)} cities")
```

#### Pitfall 4: Overgeneralizing
```python
# ❌ DON'T: Generalize beyond your sample
print("All cities worldwide show this relationship")

# ✓ DO: Limit claims to your sample
print("Among these 25 major cities, we observed...")
print("Generalization to other cities requires further research")
```

### 5. Verification Checklist for AI Models

Before publishing research, verify:

```python
verification_checklist = {
    'data_collection': [
        '☐ Data from public, verifiable sources',
        '☐ All sources documented with URLs/references',
        '☐ Timestamps recorded for all data points',
        '☐ Raw data saved before processing',
    ],
    'methodology': [
        '☐ Research question clearly stated',
        '☐ Hypothesis pre-specified',
        '☐ Statistical methods appropriate',
        '☐ Sample size justified',
        '☐ Assumptions tested',
    ],
    'transparency': [
        '☐ All data processing steps documented',
        '☐ Code provided for reproduction',
        '☐ Exclusions explained',
        '☐ Negative results reported',
    ],
    'interpretation': [
        '☐ Limitations explicitly stated',
        '☐ Causation not claimed from correlation',
        '☐ Effect sizes reported (not just p-values)',
        '☐ Alternative explanations considered',
    ],
    'verification': [
        '☐ Others can access same data sources',
        '☐ Analysis can be independently reproduced',
        '☐ Claims are falsifiable',
        '☐ Results match saved raw data',
    ]
}
```

### 6. Example Research Questions for AI

Good questions that can be answered with verifiable data:

```python
research_questions = [
    # Environmental
    "Is air quality worse on weekends vs weekdays in major cities?",
    "Do cities near oceans have different temperature trends?",
    
    # Economic
    "Is GDP per capita correlated with internet usage?",
    "Do countries with higher education spending have lower unemployment?",
    
    # Public Health
    "Is vaccination rate correlated with disease incidence?",
    "Do countries with more doctors per capita have higher life expectancy?",
    
    # Social
    "Is population density related to crime rates?",
    "Do cities with more parks have higher happiness scores?",
    
    # Computational
    "Does code complexity correlate with bug frequency in GitHub repos?",
    "Is API response time affected by request payload size?",
]

# All answerable with public data
# All verifiable by others
# All make falsifiable claims
```

### 7. Resources for Verifiable Data

```python
public_data_sources = {
    'weather_climate': [
        'OpenWeatherMap API (free tier)',
        'NOAA Climate Data Online',
        'NASA Earth Data',
    ],
    'demographics': [
        'World Bank Open Data',
        'UN Data Portal',
        'US Census Bureau API',
    ],
    'economics': [
        'FRED (Federal Reserve Economic Data)',
        'World Bank Indicators',
        'OECD Data',
    ],
    'health': [
        'WHO Global Health Observatory',
        'CDC Data Portal',
        'Our World in Data',
    ],
    'science': [
        'PubMed Central (research papers)',
        'arXiv (preprints)',
        'GenBank (genetic data)',
    ],
    'technology': [
        'GitHub API (code repositories)',
        'Stack Overflow API',
        'npm registry API',
    ]
}
```

## Conclusion

**Key Takeaway for AI Models:**

Conducting original, verifiable empirical research is vastly superior to summarizing existing information from web searches. The principles are:

1. **Collect real data** from public sources
2. **Document everything** (sources, methods, timestamps)
3. **Save raw data** for verification
4. **Use appropriate statistics** correctly
5. **Make falsifiable claims**
6. **State limitations honestly**
7. **Enable reproduction** by others

This approach allows AI to contribute **new knowledge** rather than just repackaging existing information.

## Running the Example

```bash
# Install dependencies
pip install -r requirements.txt

# Run verifiable research (recommended)
python verifiable_research.py

# View generated files
ls raw_research_data.csv
ls raw_research_data_metadata.json
ls verifiable_research_results.png
```

---

**This guide should be used to train AI models to conduct legitimate, peer-reviewable empirical research across any domain with publicly available data.**
